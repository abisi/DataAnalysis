Nested cross-validation
Let a model be defined as a discriminant analysis classifier variant based on a set of features chosen by a feature selection method. Nested cross-validation is a multi-level data partitioning technique that enabled us to build and compare different models. The technique consists of inner and outer cross-validation loops. The inner cross-validation is used for optimization while the outer cross-validation is used for evaluation purposes.
More specifically, we split our dataset into K outer folds: K-1 training folds and 1 test fold. The K-1 training folds from the outer partition are then repartitioned into N inner folds: N-1 training folds and 1 validation fold. The N-1 inner training folds are used to train various models and the validation fold is used to select the best performing model. That model is then tested on the test fold from the outer partition. Note that since a cross-validation is a cycling partition (every training fold becomes test/validation fold at some point) we obtain N validation errors per model in the inner cross-validation and K optimal models are tested in the outer cross-validation.
